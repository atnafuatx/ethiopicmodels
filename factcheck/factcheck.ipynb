{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sweetviz\n",
    "import random\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import csv\n",
    "from typing import List\n",
    "from flair.data import Token\n",
    "import sys,os\n",
    "import tqdm\n",
    "import pycld2 as cld2\n",
    "#from Levenshtein import distance \n",
    "from datetime import datetime, timedelta\n",
    "sys.path.append('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./', './data')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'config.txt'\n",
    "configParser.read(configFilePath)\n",
    "csvpath = configParser.get('paths', 'csvpath')\n",
    "outpath = configParser.get('paths', 'outpath')\n",
    "outpath,csvpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.amharicSegmenter import AmharicSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_eng(data):\n",
    "    return ' '.join(re.sub(u\"[^ሀ-፼]\", \" \", data).split()).strip()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicated_values_data(data):\n",
    "    dup=[]\n",
    "    columns=data.columns\n",
    "    for i in data.columns:\n",
    "        dup.append(sum(data[i].duplicated()))\n",
    "    return pd.concat([pd.Series(columns),pd.Series(dup)],axis=1,keys=['Columns','Duplicate count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_punct = []\n",
    "word_punct = []\n",
    "amseg = AmharicSegmenter(sent_punct,word_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get today's tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\\20-12-2020-news.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "remove = ['\\xa087','\\xa0', '|', '/','…','«','\"','“','”','»','-',\n",
    "          '$','%','^','&','*','›','’','‘','‹','<','>','`','´','~','=','+']\n",
    "outfile = outpath+\"todays_news.txt\"\n",
    "os.remove(outfile) if os.path.exists(outfile) else None\n",
    "allnews = []\n",
    "for news in glob.glob(csvpath+\"/*.csv\"):\n",
    "    allnews.append(news)\n",
    "\n",
    "with open (outfile,\"a\", encoding=\"utf-8\") as todays_news:\n",
    "    lines_seen = set() # holds lines already seen\n",
    "    for news in tqdm.tqdm(allnews,  position=0, leave=True):\n",
    "        data = pd.read_csv(news, names=[\"ID\",\"URL\",\"Date\",\"Media\",\"Content\"] ,encoding=\"utf-8\")\n",
    "        print(news)\n",
    "        break\n",
    "        todaydate = datetime.today().strftime('%Y-%m-%d') #get the date of today from server in %Y-%m-%d format\n",
    "        data = data.loc[(data['Date'] >= todaydate)]\n",
    "        #data[(data['Date'] >= todaydate)]\n",
    "        data = data[data.Content.duplicated()==False].reset_index()\n",
    "        data.Content = data.Content.apply(lambda x: remove_eng(x))\n",
    "        for token in remove:\n",
    "            data.Content = data.Content.apply(lambda x: x.replace(token,' '))\n",
    "            #data[(data['Date'] >= datetime.date(datetime.now()))]\n",
    "        \n",
    "        for text in data.Content:\n",
    "            for s in amseg.tokenize_sentence(text):\n",
    "                isReliable, textBytesFound, details  = cld2.detect(s)\n",
    "                if details[0][1] =='am' and s not in lines_seen: # check if line is not duplicate:                  \n",
    "                    todays_news.write(' '.join(t.text for t in amseg.amharic_tokenizer(s))+'\\n')\n",
    "                    lines_seen.add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "r = requests.get('http://localhost:9200') \n",
    "i = 1\n",
    "if r.status_code == 200:\n",
    "    for s in allsentence:\n",
    "        body = {\n",
    "            \"sentence\":s\n",
    "        }\n",
    "        es.index(index='amcor', doc_type='all', id=i, body=body)\n",
    "        i=i+1\n",
    "        if (i%10000 == 0):\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "file = open(\"todays_news.txt\", \"r\", encoding=\"utf-8\")\n",
    "line_count = 0\n",
    "for line in file:\n",
    "    if line != \"\\n\":\n",
    "        line_count += 1\n",
    "file.close()\n",
    "\n",
    "print(line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(['Unnamed: 0','id','location','time','user'], axis =1)\n",
    "#data['Date'] = pd.to_datetime(data['Date'], format='%Y-%m-%d')\n",
    "#todaytweet=df[(df['date'] >= datetime.today().strftime('%Y-%m-%d'))] #get the date of today from server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search by keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_sentences.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "list_of_sentences = content.splitlines()\n",
    "sentences_series = pd.Series(list_of_sentences)\n",
    "tagret_sentences = sentences_series[sentences_series.str.contains(\"ምርጫ\")] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_news = 'ታዛቢዎች ግን ምርጫዎቹ ነፃ ፍትሃዊ ስለመሆናቸው ይጠራጠራሉ'\n",
    "news=set()\n",
    "def match(target_news):\n",
    "  for new in sentences_series:\n",
    "    if distance(target_news, new) <=10:\n",
    "        news.add(new)\n",
    "match(target_news)       \n",
    "print (news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
