{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Hate speech classification using  deep learning models\n",
    "\n",
    "Resource \n",
    "https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "\n",
    " https://www.youtube.com/watch?v=2qnXmB65Ino&t=41s\n",
    " \n",
    " # Import  deep learning libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydot\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import mlxtend\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from tensorflow import keras \n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "label_encoder=preprocessing.LabelEncoder()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)#To display full length of the text\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = pd.read_csv('final_dataset.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>anno1</th>\n",
       "      <th>anno2</th>\n",
       "      <th>curator</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3029</th>\n",
       "      <td>567</td>\n",
       "      <td>1325473860804882433</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>ግዜ ለኩሉ በተራችሁ ወጣችሁ ጩሁ ማፈሪያዎች</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3030</th>\n",
       "      <td>568</td>\n",
       "      <td>1345406853719605248</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>ይቅርታ ከጎረቤቱ ውሻ የበለጠ ቆንጆ ነኝ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>569</td>\n",
       "      <td>1388926201213956102</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>ያልፈሳንበት ዳገት የለም አለች አህያ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <td>570</td>\n",
       "      <td>1358433535296761857</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>ውሻ በበላበት ቢጮህ ምን ይደንቃል ።</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3033</th>\n",
       "      <td>571</td>\n",
       "      <td>1394711686007902210</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>normal</td>\n",
       "      <td>ደደብ ነን እንጂ ድሀ አይደለንም!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0             tweet_id   anno1   anno2 curator  \\\n",
       "3029         567  1325473860804882433    hate    hate    hate   \n",
       "3030         568  1345406853719605248  normal  normal  normal   \n",
       "3031         569  1388926201213956102  normal  normal  normal   \n",
       "3032         570  1358433535296761857  normal  normal  normal   \n",
       "3033         571  1394711686007902210    hate    hate  normal   \n",
       "\n",
       "                            tweet  \n",
       "3029  ግዜ ለኩሉ በተራችሁ ወጣችሁ ጩሁ ማፈሪያዎች  \n",
       "3030    ይቅርታ ከጎረቤቱ ውሻ የበለጠ ቆንጆ ነኝ  \n",
       "3031      ያልፈሳንበት ዳገት የለም አለች አህያ  \n",
       "3032      ውሻ በበላበት ቢጮህ ምን ይደንቃል ።  \n",
       "3033        ደደብ ነን እንጂ ድሀ አይደለንም!  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove emoji and symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "                             \"]+\", flags=re.UNICODE)\n",
    "    return(emoji_pattern.sub(r'', text))\n",
    " \n",
    "df[u'tweet'] = df[u'tweet'].astype(str)\n",
    "df[u'tweet'] = df[u'tweet'].apply(lambda x:demoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv',index=False)\n",
    "test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv',sep=',', encoding='utf-8')#header=0,\n",
    "df_test = pd.read_csv('test.csv',sep=',', encoding='utf-8')#header=0,\n",
    "#df_train.info()\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns =['label', 'tweet']\n",
    "df_test.columns =['label', 'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = (df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['tweet'].value_counts()\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pad squence and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df_train['tweet'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = tokenizer.texts_to_sequences(df_train['tweet'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH) \n",
    "max_len = 0\n",
    "for t in X:\n",
    "    if max_len < len(t):\n",
    "        max_len = len(t)\n",
    "X1 = tokenizer.texts_to_sequences(df_test['tweet'].values)\n",
    "X1 = pad_sequences(X1, maxlen=max_len)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(df_train['label']).values\n",
    "Y1 = pd.get_dummies(df_test['label']).values\n",
    "print('Shape of label tensor:', Y.shape)\n",
    "Y2 = pd.get_dummies(test['label']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 8)\n",
    "X_train = X\n",
    "X_test=X1\n",
    "Y_train=Y\n",
    "Y_test=Y1\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "print(model.summary())\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)#callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictl1= model.predict(X1)\n",
    "print(valid_predictl1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions for all samples\n",
    "predictions = model.predict(X1)\n",
    "print(predictions)\n",
    "predict_resultsl1=predictions.argmax(axis=1)\n",
    "print(predict_resultsl1[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "test['predict']= predict_resultsl1\n",
    "test['predict'] = np.where((test.predict == 0),'hate',test.predict)\n",
    "test['predict'] = np.where((test.predict =='1'),'normal',test.predict)\n",
    "test['predict'] = np.where((test.predict =='2' ),'offensive',test.predict)\n",
    "test['predict'] = np.where((test.predict =='3'),'unsure',test.predict)\n",
    "labels = ['hate','normal','offensive','unsure']\n",
    "print(classification_report(test['label'].tolist(),test['predict'].tolist(),labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "    \n",
    "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "    \n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "#print(valid_predictl1[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lstm confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X1)\n",
    "pred= np.argmax(pred, axis = 1)\n",
    "test_labels = np.argmax(Y1,axis = 1)\n",
    "cm=confusion_matrix(test_labels,pred)\n",
    "plot_confusion_matrix(cm,figsize=(8,8), class_names=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lstm loss and accuracy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLstm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional,Dense,Embedding,Dropout\n",
    "\n",
    "model_b=Sequential()\n",
    "model_b.add(Embedding(70000,32,input_length=100))\n",
    "model_b.add(Dropout(0.7))\n",
    "model_b.add(Bidirectional(LSTM(100,return_sequences=True)))\n",
    "model_b.add(Bidirectional(LSTM(160)))\n",
    "model_b.add(Dense(2,activation='softmax'))\n",
    "model_b.compile(optimizer='adam',\n",
    "              #loss='binary_crossentropy',\n",
    "              loss='CategoricalCrossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(model_b.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=10\n",
    "batch_size=32\n",
    "history = model_b.fit(X_train, Y_train, batch_size=batch_size, epochs=epoch, validation_split=0.1, verbose=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model_b.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predict= model_b.predict(X1)\n",
    "print(valid_predict[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions for all samples\n",
    "predictions1 = model_b.predict(X1)\n",
    "print(predictions1)\n",
    "predict_resultsb=predictions.argmax(axis=1)\n",
    "print(predict_resultsb[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test['predict']= predict_resultsb\n",
    "test['predict'] = np.where((test.predict == 0),'hate',test.predict)\n",
    "test['predict'] = np.where((test.predict =='1'),'normal',test.predict)\n",
    "test['predict'] = np.where((test.predict =='2' ),'offensive',test.predict)\n",
    "test['predict'] = np.where((test.predict =='3'),'unsure',test.predict)\n",
    "labels = ['hate','normal','offensive','unsure']\n",
    "print(classification_report(test['label'].tolist(),test['predict'].tolist(),labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = \"@USER ለህዝባችሁ የማትራሩ ፅንፈኞች ከስካራችሁ ስትነቁ ወየው ያኔ ግፋችሁ ከትግራይ ህዝብ አልፎ ሌላውንም እያዳረሰ ነው ይብላኝ ለነገው ትውልድ ተጋሩ በናንተ ስራ ጠላት ብዙ ሆኖ አንገቱን ደፍቴ ይኖራል\"\n",
    "#model = model_b\n",
    "#yhat = model_b.predict(X)\n",
    "predb = model_b.predict(X1)\n",
    "predb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predb = model_b.predict(X1)\n",
    "predb= np.argmax(predb, axis = 1)\n",
    "test_labels = np.argmax(Y1,axis = 1)\n",
    "cm=confusion_matrix(test_labels,predb)\n",
    "plot_confusion_matrix(cm,figsize=(8,8), class_names=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "max_features =50000\n",
    "embedding_dim = 100\n",
    "sequence_length = 100\n",
    "\n",
    "model_c = tf.keras.Sequential()\n",
    "model_c.add(layers.Embedding(max_features + 1, embedding_dim, input_length=X.shape[1]))\n",
    "model_c.add(layers.Conv1D(128, 10, activation='tanh'))\n",
    "model_c.add(layers.GlobalMaxPooling1D())\n",
    "model_c.add(layers.Dense(4, activation='tanh'))\n",
    "model_c.add(layers.Dense(4, activation='sigmoid'))\n",
    "model_c.compile(optimizer='adam',\n",
    "              #loss='binary_crossentropy',\n",
    "              loss='CategoricalCrossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print(model_c.summary())\n",
    "history = model_c.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)#,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model_c.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_predict= model_c.predict(X1)\n",
    "print(valid_predict[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate predictions for all samples\n",
    "predictions_c = model_c.predict(X1)\n",
    "print(predictions_c)\n",
    "predict_results_c=predictions_c.argmax(axis=1)\n",
    "print(predict_results_c[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test['predict']= predict_results_c\n",
    "test['predict'] = np.where((test.predict == 0),'hate',test.predict)\n",
    "test['predict'] = np.where((test.predict =='1'),'normal',test.predict)\n",
    "test['predict'] = np.where((test.predict =='2' ),'offensive',test.predict)\n",
    "test['predict'] = np.where((test.predict =='3'),'unsure',test.predict)\n",
    "labels = ['hate','normal','offensive','unsure']\n",
    "print(classification_report(test['label'].tolist(),test['predict'].tolist(),labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_c1 = model_c.predict(X1)\n",
    "pred_c1= np.argmax(pred_c1, axis = 1)\n",
    "test_labels = np.argmax(Y1,axis = 1)\n",
    "cm=confusion_matrix(test_labels,pred_c1)\n",
    "plot_confusion_matrix(cm,figsize=(8,8), class_names=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cnn loss and accuracy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = model.predict(X_test) # get our predictions\n",
    "acc = accuracy_score(y_test, y_pred) \n",
    "print('Overall accuracy of RNN: {:.3f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
