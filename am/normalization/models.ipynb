{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText Model Training\n",
    "adapted from: https://github.com/Abe2G/-Learning-Word-and-Sub-word-Vectors-for-Amharic-Less-Resourced-Language-/blob/master/train_word_embedding.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import os\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "EMBEDDING_DIR='Model/'\n",
    "PREPROCESSED_DIR='processed/'\n",
    "class WordEmbeddingConfig(object):\n",
    "    \"\"\"Word2Vec Training parameters\"\"\"\n",
    "    window=10 #Maximum skip length window between words\n",
    "    emb_dim=200 # Set size of word vectors\n",
    "    emb_lr=0.05 #learning rate for SGD estimation.\n",
    "    nepoach=20 #number of training epochs\n",
    "    nthread=100 #number of training threads\n",
    "    sample = 0 #Set threshold for occurrence of words. Those that appear with higher frequency in the training data will be randomly down-sampled\n",
    "    negative = 15 #negative sampling is used with defined negative example\n",
    "    hs = 0 #0 Use Hierarchical Softmax; default is 0 (not used)\n",
    "    binary=0 # 0 means not saved as .bin. Change to 1 if allowed to binary format\n",
    "    sg=0 # 0 means CBOW model is used. Change to 1 to use Skip-gram model\n",
    "    iterate=10 # Run more training iterations\n",
    "    minFreq=1 #This will discard words that appear less than minFreq times \n",
    "    if sg==0:\n",
    "        model_name='am_fasttext_cbow_'+str(emb_dim)+'D'\n",
    "    elif sg==1:\n",
    "         model_name='am_fasttext_sg_'+str(emb_dim)+'D'\n",
    "\n",
    "class corpus_sentences(object):# accept sentence stored one per line in list of files inside defined directory\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname),encoding='utf8'):\n",
    "                yield line.split()\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)\n",
    "\n",
    "\n",
    "def load_am_word_vectors():\n",
    "    if WordEmbeddingConfig.sg==0:\n",
    "            model_type='CBOW'\n",
    "    else:\n",
    "        model_type='Skip-gram'        \n",
    "    if os.path.exists(WordEmbeddingConfig.model_name):\n",
    "        print('Loading Word2Vec Amharic Pretrained '+model_type+' model with '+str(WordEmbeddingConfig.emb_dim)+' dimension\\n') \n",
    "        am_model= KeyedVectors.load(WordEmbeddingConfig.model_name)\n",
    "    else:\n",
    "        print('Loading Sentences with memory freindly iterator ...\\n')\n",
    "        sentences = corpus_sentences(PREPROCESSED_DIR) # a memory-friendly iterator        \n",
    "        print('Training FastText '+model_type+' with '+str(WordEmbeddingConfig.emb_dim)+' dimension\\n') \n",
    "        am_model = FastText(size=WordEmbeddingConfig.emb_dim, window=WordEmbeddingConfig.window, \n",
    "                            min_count=WordEmbeddingConfig.minFreq, workers=WordEmbeddingConfig.nthread,sg=WordEmbeddingConfig.sg,\n",
    "                            iter=WordEmbeddingConfig.iterate,negative=WordEmbeddingConfig.negative,\n",
    "                            hs=WordEmbeddingConfig.hs)\n",
    "        am_model.build_vocab(sentences)\n",
    "\n",
    "        am_model.train(sentences, total_examples=am_model.corpus_count, epochs=am_model.iter)\n",
    "        #trim unneeded model memory = use (much) less RAM\n",
    "        am_model.init_sims(replace=True)\n",
    "        \n",
    "        #Saving model    \n",
    "        model_name=EMBEDDING_DIR+WordEmbeddingConfig.model_name\n",
    "        am_model.save(model_name)        \n",
    "        \n",
    "    return am_model \n",
    "# uncomment the following to start training\n",
    "am_model=load_am_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "am_model= KeyedVectors.load('Model/am_fasttext_cbow_200D')\n",
    "print(am_model.wv.doesnt_match(\"አንድ ሺህ ሚሊዮን አምስት ብዙ ጅማ\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_model.most_similar('ሐገር')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import os\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "EMBEDDING_DIR='Model/'\n",
    "PREPROCESSED_DIR='processed/'\n",
    "class WordEmbeddingConfig(object):\n",
    "    \"\"\"Word2Vec Training parameters\"\"\"\n",
    "    window=5 #Maximum skip length window between words\n",
    "    emb_dim=200 # Set size of word vectors\n",
    "    emb_lr=0.05 #learning rate for SGD estimation.\n",
    "    nepoach=10 #number of training epochs\n",
    "    nthread=100 #number of training threads\n",
    "    sample = 0 #Set threshold for occurrence of words. Those that appear with higher frequency in the training data will be randomly down-sampled\n",
    "    negative = 10 #negative sampling is used with defined negative example\n",
    "    hs = 0 #0 Use Hierarchical Softmax; default is 0 (not used)\n",
    "    binary=0 # 0 means not saved as .bin. Change to 1 if allowed to binary format\n",
    "    sg=0 # 0 means CBOW model is used. Change to 1 to use Skip-gram model\n",
    "    iterate=20 # Run more training iterations\n",
    "    minFreq=5 #This will discard words that appear less than minFreq times \n",
    "    if sg==0:\n",
    "        model_name='5w_10ng_am_w2v_cbow_'+str(emb_dim)+'D'\n",
    "    elif sg==1:\n",
    "         model_name='5w_10ng_am_w2v_sg_'+str(emb_dim)+'D'\n",
    "\n",
    "class corpus_sentences(object):# accept sentence stored one per line in list of files inside defined directory\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname),encoding='utf8'):\n",
    "                yield line.split()\n",
    "    \n",
    "def train_w2v_model():\n",
    "    print('Loading Sentences with memory freindly iterator ...\\n')\n",
    "    sentences = corpus_sentences(PREPROCESSED_DIR) # a memory-friendly iterator \n",
    "    if WordEmbeddingConfig.sg==0:\n",
    "            model_type='CBOW'\n",
    "    else:\n",
    "        model_type='Skip-gram'    \n",
    "    print('Training Word2Vec '+model_type+' with '+str(WordEmbeddingConfig.emb_dim)+' dimension\\n') \n",
    "    _model = Word2Vec(sentences, size=WordEmbeddingConfig.emb_dim, window=WordEmbeddingConfig.window, \n",
    "                            min_count=WordEmbeddingConfig.minFreq, workers=WordEmbeddingConfig.nthread,sg=WordEmbeddingConfig.sg,\n",
    "                            iter=WordEmbeddingConfig.iterate,negative=WordEmbeddingConfig.negative,sample=WordEmbeddingConfig.sample,\n",
    "                            hs=WordEmbeddingConfig.hs,sorted_vocab=1)\n",
    "  \n",
    "    #trim unneeded model memory = use (much) less RAM\n",
    "    _model.init_sims(replace=True)\n",
    "    \n",
    "    #Saving model   \n",
    "    model_name=EMBEDDING_DIR+WordEmbeddingConfig.model_name\n",
    "    _model.save(model_name)\n",
    "\n",
    "    return _model        \n",
    "train_w2v_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "am_model= KeyedVectors.load('Model/5w_10ng_am_w2v_cbow_200D')\n",
    "print(am_model.wv.doesnt_match(\"አንድ ሺህ ሚሊዮን አምስት ብዙ ጅማ\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec using SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm,glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(input=glob.glob('processed/*.txt'), vocab_size=32000, model_prefix='amh_sp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import os\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "import sentencepiece as spm\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "EMBEDDING_DIR='Model/'\n",
    "PREPROCESSED_DIR='processed/'\n",
    "class WordEmbeddingConfig(object):\n",
    "    \"\"\"Word2Vec Training parameters\"\"\"\n",
    "    window=5 #Maximum skip length window between words\n",
    "    emb_dim=100 # Set size of word vectors\n",
    "    emb_lr=0.05 #learning rate for SGD estimation.\n",
    "    nepoach=20 #number of training epochs\n",
    "    nthread=100 #number of training threads\n",
    "    sample = 0 #Set threshold for occurrence of words. Those that appear with higher frequency in the training data will be randomly down-sampled\n",
    "    negative = 15 #negative sampling is used with defined negative example\n",
    "    hs = 0 #0 Use Hierarchical Softmax; default is 0 (not used)\n",
    "    binary=0 # 0 means not saved as .bin. Change to 1 if allowed to binary format\n",
    "    sg=1 # 0 means CBOW model is used. Change to 1 to use Skip-gram model\n",
    "    iterate=10 # Run more training iterations\n",
    "    minFreq=5 #This will discard words that appear less than minFreq times \n",
    "    if sg==0:\n",
    "        model_name='sp_am_w2v_cbow_'+str(emb_dim)+'D'\n",
    "    elif sg==1:\n",
    "         model_name='sp_am_w2v_sg_'+str(emb_dim)+'D'\n",
    "\n",
    "class corpus_sentences(object):# accept sentence stored one per line in list of files inside defined directory\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        self.sp=spm.SentencePieceProcessor()\n",
    "        self.sp_model=self.sp.Load(\"amh_sp.model\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname),encoding='utf8'):\n",
    "                yield self.sp.EncodeAsPieces(line)\n",
    "    \n",
    "def train_w2v_model():\n",
    "    print('Loading Sentences with memory freindly iterator ...\\n')\n",
    "    sentences = corpus_sentences(PREPROCESSED_DIR) # a memory-friendly iterator \n",
    "    if WordEmbeddingConfig.sg==0:\n",
    "            model_type='CBOW'\n",
    "    else:\n",
    "        model_type='Skip-gram'    \n",
    "    print('Training Sentence Piece Word2Vec '+model_type+' with '+str(WordEmbeddingConfig.emb_dim)+' dimension\\n') \n",
    "    _model = Word2Vec(sentences, size=WordEmbeddingConfig.emb_dim, window=WordEmbeddingConfig.window, \n",
    "                            min_count=WordEmbeddingConfig.minFreq, workers=WordEmbeddingConfig.nthread,sg=WordEmbeddingConfig.sg,\n",
    "                            iter=WordEmbeddingConfig.iterate,negative=WordEmbeddingConfig.negative,\n",
    "                            hs=WordEmbeddingConfig.hs,sorted_vocab=1)\n",
    "  \n",
    "    #trim unneeded model memory = use (much) less RAM\n",
    "    _model.init_sims(replace=True)\n",
    "    \n",
    "    #Saving model   \n",
    "    model_name=EMBEDDING_DIR+WordEmbeddingConfig.model_name\n",
    "    _model.save(model_name)\n",
    "\n",
    "    return _model        \n",
    "train_w2v_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "am_model= KeyedVectors.load('Model/sp_am_w2v_cbow_100D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Vocab: ',len(am_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_model.most_similar('በሬ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword embedding from pretrained BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "bpemb_en = BPEmb(lang=\"am\", dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemb_am = BPEmb(lang=\"am\", vs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemb_am.encode('የአገልግሎቶችና')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemb_am.most_similar('በሬ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
