{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Tadesse-Destaw/Normalization/blob/main/NLP_Preprocessing_and_Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lk-54TsjgNLU"
   },
   "source": [
    "# Amharic Text Preprocessing & Tokinization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flair\n",
    "!pip install sweetviz\n",
    "!pip install Unidecode\n",
    "!pip install emoji\n",
    "!pip install python-Levenshtein\n",
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "nvd09op035BW"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "from IPython.display import display\n",
    "import sweetviz\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import glob\n",
    "import csv\n",
    "import emoji\n",
    "from typing import List\n",
    "from flair.data import Token\n",
    "from Levenshtein import distance \n",
    "import import_ipynb\n",
    "from preprocessing.amharicSegmenter import AmharicSegmenter\n",
    "#from Segmentation import remove_url\n",
    "from normalizer import normalize\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url(string): \n",
    "    text = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',string)\n",
    "    return \"\".join(text) # converting return value from list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "q4Zkol-9bT3g"
   },
   "outputs": [],
   "source": [
    "def remove_url(data):\n",
    "    return data.replace(find_url(data),\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "tnGIUSmjZA8d"
   },
   "outputs": [],
   "source": [
    "def remove_English_letters(news):\n",
    "    for idx,cnt in enumerate(news):\n",
    "        news[idx] =re.sub(r'[A-Za-z]','', cnt)  #\\s+\n",
    "    return news\n",
    "\n",
    "def normalize(data):\n",
    "    normal = unicodedata.normalize('NFKD', data).encode('ASCII', 'ignore')\n",
    "    return(normal.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "OfXuHExklhva"
   },
   "outputs": [],
   "source": [
    "def remove_emoji(news):\n",
    "    for idx,cnt in enumerate(news):\n",
    "        news[idx] = re.sub(r'(:\\S+)(@\\w+)','', cnt)\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "6sWjOiofABCZ"
   },
   "outputs": [],
   "source": [
    "def duplicated_values_data(data):\n",
    "    dup=[]\n",
    "    columns=data.columns\n",
    "    for i in data.columns:\n",
    "        dup.append(sum(data[i].duplicated()))\n",
    "    return pd.concat([pd.Series(columns),pd.Series(dup)],axis=1,keys=['Columns','Duplicate count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0X15p4qZbFr",
    "outputId": "be331b28-a77c-4113-a512-7272bfc8709d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Removing Duplicated Sentences\n",
      "   Columns  Duplicate count\n",
      "0  Content              280\n",
      "======================================== \n",
      "After Removing Duplicated Sentences\n",
      "   Columns  Duplicate count\n",
      "0  Content                0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-1b811959e141>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#Remove Latin-alphabets from content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mMay_news_DF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMay_news_DF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mremove_English_letters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4106\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4107\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4108\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-105-1b811959e141>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m#Remove Latin-alphabets from content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mMay_news_DF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMay_news_DF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mremove_English_letters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-104-f5f797fb0d94>\u001b[0m in \u001b[0;36mremove_English_letters\u001b[1;34m(news)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mremove_English_letters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcnt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mnews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\s*[A-Za-z]+\\b'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnt\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#\\s+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnews\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "remove = ['\\xa087','\\n','\\xa0', '|', '/','፤','…','፣','«','(',')','\"','“','”','»','-','.',\n",
    "          '@','#','$','%','^','&','*','[',']','{','}',';','›','’','‘','‹','<','>','`','´','~','=','+','፦','፥','፧','፠']\n",
    "May_news_DF = pd.DataFrame()\n",
    "#May_news  = glob.glob(\"/srv/data/amharic/crawl/output/*.csv\")\n",
    "#May_news  = glob.glob(\"/srv/data/amharic/crawl/output/2020-1-28-0-0-3.csv\")\n",
    "May_news  = glob.glob(\"data/*.csv\")\n",
    "for news in May_news:\n",
    "    data = pd.read_csv(news, names=[\"ID\",\"URL\",\"Date\",\"Media\",\"Content\"] ,encoding=\"utf-8\")\n",
    "    #Removing duplicated sentences from Content\n",
    "    data = data[data.Content.duplicated()==False].reset_index()\n",
    "   \n",
    "    for token in remove:\n",
    "        data.Content = data.Content.apply(lambda x: x.replace(token,' ')) \n",
    "    #Removing the url from Content\n",
    "    data.Content = data.Content.apply(lambda x: remove_url(x))\n",
    "    data = data.drop(['index','ID','URL','Date','Media'], axis=1)\n",
    "    #Merging the Dataframes\n",
    "    May_news_DF = pd.concat([May_news_DF,data])\n",
    "\n",
    "print(\"Before Removing Duplicated Sentences\")\n",
    "print(duplicated_values_data(May_news_DF))\n",
    "\n",
    "May_news_DF = May_news_DF[May_news_DF.Content.duplicated()==False].reset_index()\n",
    "May_news_DF = May_news_DF.drop(['index'], axis=1)\n",
    "print(\"=\"*40,\"\\nAfter Removing Duplicated Sentences\")\n",
    "print(duplicated_values_data(May_news_DF))\n",
    "\n",
    "#Splitting the sentences\n",
    "#May_news_DF.Content = May_news_DF.Content.apply(lambda x: sentence_splitter(x))\n",
    "\n",
    "#Remove Latin-alphabets from content \n",
    "May_news_DF.Content = May_news_DF.Content.apply(lambda x: remove_English_letters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x32mORWblTT4",
    "outputId": "2b09a46a-782c-4810-9d0c-a54470b05213"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5873488"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of Sentences\n",
    "Sentence_Number = []\n",
    "for x in May_news_DF.Content:\n",
    "    Sentence_Number.append(len(x))\n",
    "\n",
    "sum(Sentence_Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "hGCdC9YCajga"
   },
   "outputs": [],
   "source": [
    "#May_news_DF.to_csv(\"MayNews.csv\")\n",
    "May_news_DF.Content.to_csv(\"MayNews_Content.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "TnGnSUuWva-X",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#May_news_DF.Content[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUxQey0--inh"
   },
   "source": [
    "# Generate Homophone words from the corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCJQmsCQfF0V"
   },
   "outputs": [],
   "source": [
    "hm1 =['ሀ','ሃ','ኀ','ኃ','ሐ','ሓ']\n",
    "hm2 =['ሁ','ሑ','ኁ']\n",
    "hm3 =['ህ','ሕ','ኅ']\n",
    "hm4 =['ሰ','ሠ']\n",
    "hm5 =['ስ','ሥ']\n",
    "hm6 =['አ','ኣ','ዐ','ዓ']\n",
    "hm7 =['ፀ','ጸ']\n",
    "hm8 =['ፅ','ጽ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "E2hwG9C4Wkfr"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"MayNews_Content.csv\")\n",
    "df=pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJl7E1jJiMv5"
   },
   "outputs": [],
   "source": [
    "#Converting Dataframe to Lists\n",
    "#News = df[df.Content.str.contains('|'.join(hm1))]\n",
    "#News['Content'] = News.Content.apply(lambda x: str(x).split())\n",
    "list_of_words = df['Content'].str.split(' ').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rb0FCGx2QEl",
    "outputId": "1d1db810-e4c8-49fe-d9c2-414e4e783ef4"
   },
   "outputs": [],
   "source": [
    "print (len(list_of_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71_a57xKon_v"
   },
   "outputs": [],
   "source": [
    "#Converting Lists to Pandas Series\n",
    "pd_series = pd.Series(list_of_words)\n",
    "pd_series = pd_series.apply(pd.Series).stack().reset_index(drop = True) \n",
    "series_List = pd_series[pd_series.str.contains('|'.join(hm1))]            #words contains hml only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2626tXKPN3-X",
    "outputId": "ada14491-fd9b-4e43-a19f-22b5f5ffe7b1"
   },
   "outputs": [],
   "source": [
    "series_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dr7VyhmYv3G3",
    "outputId": "cfa19fe5-82d6-46da-9d6b-e26d3431ceaa"
   },
   "outputs": [],
   "source": [
    "#Remove duplicated Valuees\n",
    "test_words = set(series_List)\n",
    "print (len(test_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9Z1n46UKb_by",
    "outputId": "daa20407-8f3f-404b-84da-0a5d17ab8dcf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hm1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-a6fd47c76e4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtarget_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ሀገር'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mhm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhm1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m   \u001b[0mposition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mposition\u001b[0m \u001b[1;33m>=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hm1' is not defined"
     ]
    }
   ],
   "source": [
    "target_word = 'ሀገር'\n",
    "for hm in hm1:\n",
    "  position=target_word.index(hm)\n",
    "  if position >=0:\n",
    "    break\n",
    "target_word[:position] + '' + target_word[position+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1M4U9YKOc3d6",
    "outputId": "515055b4-baf7-4c6d-9b50-4afd7e361ccd"
   },
   "outputs": [],
   "source": [
    "words=set()\n",
    "def match(target_word):\n",
    "  for word in test_words:\n",
    "    search_word =word.replace('|'.join(hm1),'')\n",
    "    if distance(target_word, search_word) <=1:\n",
    "      words.add(search_word)\n",
    "match(target_word)       \n",
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "3Ve83gnRRAy1",
    "outputId": "c312102b-d281-4bbb-aa7d-84e8d4b24024"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "series_List1 = pd_series[pd_series.str.contains('ሀ')] \n",
    "ሀ=len(series_List1)\n",
    "series_List2 = pd_series[pd_series.str.contains('ሐ')] \n",
    "ሐ=len(series_List2)\n",
    "series_List3 = pd_series[pd_series.str.contains('ኀ')] \n",
    "ኀ=len(series_List3)\n",
    "series_List4 = pd_series[pd_series.str.contains('ሃ')] \n",
    "ሃ=len(series_List4)\n",
    "series_List5 = pd_series[pd_series.str.contains('ሓ')] \n",
    "ሓ=len(series_List5)\n",
    "series_List6 = pd_series[pd_series.str.contains('ኃ')] \n",
    "ኃ=len(series_List6)\n",
    "data = {}\n",
    "for variable in [\"ሀ\", \"ሐ\", \"ኀ\",\"ሃ\",\"ሓ\",\"ኃ\"]:\n",
    "    data[variable] = eval(variable)\n",
    "import operator\n",
    "sorted_d = sorted(data.items(), key=operator.itemgetter(1))\n",
    "characters = list(data.keys()) \n",
    "no_words = list(data.values()) \n",
    "   \n",
    "fig = plt.figure(figsize = (10, 6)) \n",
    "# creating the bar plot \n",
    "plt.bar(characters, no_words, color ='maroon') \n",
    "  \n",
    "plt.xlabel(\"Homophone (ha) sound characters\") \n",
    "plt.ylabel(\"Frequency of words\") \n",
    "plt.title(\"Distribution of characters in words\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK-m1_eAvRjq"
   },
   "source": [
    "## Homophone word Occurance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-IfUg9E4O1k",
    "outputId": "50ab21f6-2c43-4341-eab0-6c9275af502c"
   },
   "outputs": [],
   "source": [
    "homo_word = 'ሐገር'\n",
    "result = list(filter(lambda x: homo_word in x, series_List))\n",
    "#result = [i for i in series_List if homo_word in i] \n",
    "print (len(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZjC1Yq5-2IB"
   },
   "source": [
    "## Print homophone words with context (the whole sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "9ySSfk1KJim3"
   },
   "outputs": [],
   "source": [
    "f = open(\"all_sentences.txt\", \"r\",encoding=\"utf-8\")\n",
    "content = f.read()\n",
    "list_of_sentences = content.splitlines()\n",
    "f.close()\n",
    "#df = pd.read_csv(\"all_sentences.txt\")\n",
    "#News['Content'] = News.Content.apply(lambda x: str(x).split('([፡፡።?!])'))\n",
    "#list_of_sentences = df['Content'].str.split('([፡፡።?!])').to_list()\n",
    "sentences_series = pd.Series(list_of_sentences)\n",
    "sentences_series = sentences_series.apply(pd.Series).stack().reset_index(drop = True) \n",
    "tagret_sentences = sentences_series[sentences_series.str.contains(\"ሀገር\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5iWVeQxKtoX",
    "outputId": "1909c68e-4105-40c6-c4c4-b5ee122cc441"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        በአስር ሺዎች የሚቆጠሩት ሰዎች ቀያቸውን ለቀው ወደ ጎረቤት ሀገር ወደ ሱ...\n",
       "18       መጭው አዲስ ዓመት 2021 እና አፍሪቃ በአስቸጋሪ ሁኔታ የሚካሄዱ ምርጫዎ...\n",
       "106      የቀድሞ የደርግ ባለስልጣናት ከ30 ዓመት በኋላ ተናገሩ በተለይ ለአዲስ አ...\n",
       "578      በብልፅግና የሚመራው መንግስት ህዝብን ከህዝብ ለማጋጨት በሃይማኖታዊ እና ...\n",
       "584      በብልፅግና የሚመራው መንግስት ህዝብን ከህዝብ ለማጋጨት በሃይማኖታዊ እና ...\n",
       "                               ...                        \n",
       "69209    እነዚህ ሁለት መሰረታዊ ጉዳዮች ማንም በቀላሉ ሊታዘበው የሚችልና እነሱም ...\n",
       "69217    የቅናት ስሜት የተሰማኝ ለወገኖች ለሀገሬም ሆነ ለመንግስት እያበረከትኩት ...\n",
       "69236    ማህበራዊ ኑሮአችን ግድ የሚሉንና ለተለያዩ የሀገር ግንባታ አላማዎች ከመን...\n",
       "69243    ዳያስፖራዎች ካሉበት ሀገር እዚህ አገር ቤት ላሉት ዘመድ ወዳጆቻቸው የሚል...\n",
       "69249    ከእኛ ሀገር ዳያስፖራዎች ይህን ያህል ዶላር በየወሩ የሚልክ ምን ያህሉ ይሆን?\n",
       "Length: 713, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagret_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWuN7MEPhFfN"
   },
   "outputs": [],
   "source": [
    "#homo_word = df[df['Content'].str.contains(\"ድህነት\")]\n",
    "#print(\"ድህነት\", len(homo_word))\n",
    "#ድህነት,ኅብር,ሕብር,ሳለ,ሣለ,ሰረገ,ሠረገ,ሰየመ,ሠየመ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbpuMdBpuj26"
   },
   "source": [
    "## Homophone Characters Occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6wZ2JdFuiFC",
    "outputId": "7aa3d5cb-00ff-4066-aaee-0bca3e11aa52"
   },
   "outputs": [],
   "source": [
    "#ሀ,ሃ,ኀ,ኃ,ሐ,ህ,ሕ,ኅ,አ,ዐ,ዓ,ሰ,ሠ,ሳ,ሣ,ስ,ሥ,ጸ,ፀ\n",
    "hom_char = 0\n",
    "for item in series_List:\n",
    "    if 'ሃ' in item:\n",
    "        hom_char = hom_char + 1\n",
    "print(hom_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-F2kQvEpJSc"
   },
   "source": [
    "# Normalize based on frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncX7PN-4j9aw"
   },
   "outputs": [],
   "source": [
    "def normalizef(Content):\n",
    "    for idx,cnt in enumerate(Content):\n",
    "     # if len(homo_word)>len(homo_char_ha)\n",
    "        Content[idx] = re.sub(r'[ሀኀኃሐሓኻ]','ሃ', cnt)\n",
    "        Content[idx] = re.sub(r'[ጸ]','ፀ', cnt)\n",
    "  return Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lyl3NrNGkI7q"
   },
   "outputs": [],
   "source": [
    "May_news_DF.Content = May_news_DF.Content.apply(lambda x: normalizef(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQUk1nmvv7U4"
   },
   "source": [
    "# Normalize based on litrature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yJy8FzbJ3SF"
   },
   "outputs": [],
   "source": [
    "May_news_DF.Content = May_news_DF.Content.apply(lambda x: normalize(x))\n",
    "#May_news_DF.Content[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQUk1nmvv7U4"
   },
   "source": [
    "# Amharic Text Segmentation (sentence and tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_punct = []\n",
    "word_punct = []\n",
    "amseg = AmharicSegmenter(sent_punct,word_punct)\n",
    "#text=pd.DataFrame(df)\n",
    "text = df.to_string(index = False) \n",
    "#text  =  \"በዚህም መሠረት የአማራ:: ዴሞክራሲያዊ ኃይል ንቅናቄ የመረጠው: ምልክት ከዚህ በፊት በነበረ፡፡ ምርጫ ሌላ ፓርቲ ሲጠቀምበት የነበረ በመሆኑና ፓርቲው፡፡ የቀድሞ ምልክቱን ለመጠቀም በመጠየቁ፣ የጋሞ ዴሞክራሲያዊ ፓርቲ የመረጠው ምልክት ከዚህ በፊት በነበረ ምርጫ ሌላ ፓርቲ ሲጠቀምበት የነበረ በመሆኑና ፓርቲው የቀድሞ ምልክቱን ለመጠቀም በመጠየቁ፣ የኢትዮጵያ ዜጎች ለማኅበራዊ ፍትሕ (ኢዜማ) የመረጠው ምልክት ከዚህ በፊት በነበረ ምርጫ ሌላ ፓርቲ ሲጠቀምበት የነበረ በመሆኑና ፓርቲው የቀድሞ ምልክቱን ለመጠቀም በመጠየቁ፣ የምዕራብ ሶማሌ ዴሞክራሲያዊ ፓርቲ ከሌላ ፓርቲ ምልክት ጋር ምልክቱ የተቀራረበ በመሆኑ፣ እንዲሁም የአርጎባ ብሔረሰብ ዴሞክራሲያዊ ንቅናቄ ከሌላ ፓርቲ ምልክት ጋር ምልክቱ የተቀራረበ በመሆኑ ነው እንዲቀይሩ በምርጫ ቦርድ ማሳሰቢያ የተሰጣቸው፡፡\"\n",
    "with open (\"all_sentences.txt\",\"w\", encoding=\"utf-8\") as all_sentences:\n",
    "  for s in amseg.tokenize_sentence(text):\n",
    "      all_sentences.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text  =  \"በዚህም መሠረት የአማራ:: ዴሞክራሲያዊ ኃይል ንቅናቄ የመረጠው: ምልክት ከዚህ በፊት በነበረ፡፡ ምርጫ ሌላ ፓርቲ ሲጠቀምበት የነበረ በመሆኑና ፓርቲው፡፡ የቀድሞ ምልክቱን ለመጠቀም በመጠየቁ፣ የጋሞ ዴሞክራሲያዊ ፓርቲ የመረጠው ምልክት ከዚህ በፊት በነበረ ምርጫ ሌላ ፓርቲ ሲጠቀምበት የነበረ በመሆኑና ፓርቲው የቀድሞ ምልክቱን ለመጠቀም በመጠየቁ፣ የኢትዮጵያ ዜጎች ለማኅበራዊ ፍትሕ (ኢዜማ) የመረጠው ምልክት ከዚህ በፊት በነበረ ምርጫ ሌላ ፓርቲ ሲጠቀምበት የነበረ በመሆኑና ፓርቲው የቀድሞ ምልክቱን ለመጠቀም በመጠየቁ፣ የምዕራብ ሶማሌ ዴሞክራሲያዊ ፓርቲ ከሌላ ፓርቲ ምልክት ጋር ምልክቱ የተቀራረበ በመሆኑ፣ እንዲሁም የአርጎባ ብሔረሰብ ዴሞክራሲያዊ ንቅናቄ ከሌላ ፓርቲ ምልክት ጋር ምልክቱ የተቀራረበ በመሆኑ ነው እንዲቀይሩ በምርጫ ቦርድ ማሳሰቢያ የተሰጣቸው፡፡\"\n",
    "#with open (\"all_tokens.txt\",\"w\", encoding=\"utf-8\") as all_tokens:\n",
    "amseg = AmharicSegmenter(sent_punct,word_punct)\n",
    "amseg.amharic_tokenizer(text)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NLP Preprocessing and Normalization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
