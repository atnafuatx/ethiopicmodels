{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sweetviz\n",
    "import random\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import csv\n",
    "import emoji\n",
    "from typing import List\n",
    "from flair.data import Token\n",
    "import sys,os\n",
    "import tqdm\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.amharicSegmenter import AmharicSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url(string): \n",
    "    text = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',string)\n",
    "    return \"\".join(text) # converting return value from list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(data):\n",
    "    return data.replace(find_url(data),\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(news):\n",
    "    for idx,cnt in enumerate(news):\n",
    "        news[idx] = re.sub(r'(:\\S+)(@\\w+)[A-Za-z]','', cnt)\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicated_values_data(data):\n",
    "    dup=[]\n",
    "    columns=data.columns\n",
    "    for i in data.columns:\n",
    "        dup.append(sum(data[i].duplicated()))\n",
    "    return pd.concat([pd.Series(columns),pd.Series(dup)],axis=1,keys=['Columns','Duplicate count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Removing Duplicated Sentences\n",
      "   Columns  Duplicate count\n",
      "0  Content              280\n",
      "======================================== \n",
      "After Removing Duplicated Sentences\n",
      "   Columns  Duplicate count\n",
      "0  Content                0\n"
     ]
    }
   ],
   "source": [
    "remove = ['\\xa087','\\n','\\xa0', '|', '/','፤','…','፣','«','(',')','\"','“','”','»','-','.',',',\n",
    "          '@','#','$','%','^','&','*','[',']','{','}',';','›','’','‘','‹','<','>','`','´','~','=','+','፦','፥','፧','፠']\n",
    "May_news_DF = pd.DataFrame()\n",
    "#May_news  = glob.glob(\"/srv/data/amharic/crawl/output/*.csv\")\n",
    "#May_news  = glob.glob(\"/srv/data/amharic/crawl/output/2020-1-28-0-0-3.csv\")\n",
    "May_news  = glob.glob(\"data/*.csv\")\n",
    "for news in May_news:\n",
    "    data = pd.read_csv(news, names=[\"ID\",\"URL\",\"Date\",\"Media\",\"Content\"] ,encoding=\"utf-8\")\n",
    "    #Removing duplicated sentences from Content\n",
    "    data = data[data.Content.duplicated()==False].reset_index()\n",
    "   \n",
    "    for token in remove:\n",
    "        data.Content = data.Content.apply(lambda x: x.replace(token,' ')) \n",
    "    #Removing the url from Content\n",
    "    data.Content = data.Content.apply(lambda x: remove_url(x))\n",
    "    data = data.drop(['index','ID','URL','Date','Media'], axis=1)\n",
    "    #Merging the Dataframes\n",
    "    May_news_DF = pd.concat([May_news_DF,data])\n",
    "\n",
    "print(\"Before Removing Duplicated Sentences\")\n",
    "print(duplicated_values_data(May_news_DF))\n",
    "\n",
    "May_news_DF = May_news_DF[May_news_DF.Content.duplicated()==False].reset_index()\n",
    "May_news_DF = May_news_DF.drop(['index'], axis=1)\n",
    "print(\"=\"*40,\"\\nAfter Removing Duplicated Sentences\")\n",
    "print(duplicated_values_data(May_news_DF))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "May_news_DF.Content.to_csv(\"MayNews_Content.csv\", index=False)\n",
    "df = pd.read_csv(\"MayNews_Content.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amharic Text Segmentation (sentence and tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_punct = []\n",
    "word_punct = []\n",
    "amseg = AmharicSegmenter(sent_punct,word_punct)\n",
    "#text=pd.DataFrame(df)\n",
    "text = df.to_string(index = False) \n",
    "#text  =  \"በዚህም መሠረት የአማራ:: ዴሞክራሲያዊ ኃይል ንቅናቄ የመረጠው: ምልክት ከዚህ በፊት በነበረ፡፡ ምርጫ ሌላ ፓርቲ ሲጠቀምበት የነበረ በመሆኑና ፓርቲው፡፡ የቀድሞ ምልክቱን ለመጠቀም በመጠየቁ፣ የጋሞ ዴሞክራሲያዊ ፓርቲ የመረጠው ምልክት ከዚህ በፊት በነበረ ምርጫ ሌላ ፓርቲ ሲጠቀምበት የነበረ በመሆኑና ፓርቲው የቀድሞ ምልክቱን ለመጠቀም በመጠየቁ፣ የኢትዮጵያ ዜጎች ለማኅበራዊ ፍትሕ (ኢዜማ) የመረጠው ምልክት ከዚህ በፊት በነበረ ምርጫ ሌላ ፓርቲ ሲጠቀምበት የነበረ በመሆኑና ፓርቲው የቀድሞ ምልክቱን ለመጠቀም በመጠየቁ፣ የምዕራብ ሶማሌ ዴሞክራሲያዊ ፓርቲ ከሌላ ፓርቲ ምልክት ጋር ምልክቱ የተቀራረበ በመሆኑ፣ እንዲሁም የአርጎባ ብሔረሰብ ዴሞክራሲያዊ ንቅናቄ ከሌላ ፓርቲ ምልክት ጋር ምልክቱ የተቀራረበ በመሆኑ ነው እንዲቀይሩ በምርጫ ቦርድ ማሳሰቢያ የተሰጣቸው፡፡\"\n",
    "with open (\"all_sentences.txt\",\"w\", encoding=\"utf-8\") as all_sentences:\n",
    "  for s in amseg.tokenize_sentence(text):\n",
    "      all_sentences.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text  =  \"በዚህም መሠረት የአማራ:: ዴሞክራሲያዊ ኃይል ንቅናቄ የመረጠው: ምልክት ከዚህ በፊት በነበረ፡፡ ምርጫ ሌላ ፓርቲ ሲጠቀምበት የነበረ በመሆኑና ፓርቲው፡፡ የቀድሞ ምልክቱን ለመጠቀም በመጠየቁ፣ የጋሞ ዴሞክራሲያዊ ፓርቲ የመረጠው ምልክት ከዚህ በፊት በነበረ ምርጫ ሌላ ፓርቲ ሲጠቀምበት የነበረ በመሆኑና ፓርቲው የቀድሞ ምልክቱን ለመጠቀም በመጠየቁ፣ የኢትዮጵያ ዜጎች ለማኅበራዊ ፍትሕ (ኢዜማ) የመረጠው ምልክት ከዚህ በፊት በነበረ ምርጫ ሌላ ፓርቲ ሲጠቀምበት የነበረ በመሆኑና ፓርቲው የቀድሞ ምልክቱን ለመጠቀም በመጠየቁ፣ የምዕራብ ሶማሌ ዴሞክራሲያዊ ፓርቲ ከሌላ ፓርቲ ምልክት ጋር ምልክቱ የተቀራረበ በመሆኑ፣ እንዲሁም የአርጎባ ብሔረሰብ ዴሞክራሲያዊ ንቅናቄ ከሌላ ፓርቲ ምልክት ጋር ምልክቱ የተቀራረበ በመሆኑ ነው እንዲቀይሩ በምርጫ ቦርድ ማሳሰቢያ የተሰጣቸው፡፡\"\n",
    "#with open (\"all_tokens.txt\",\"w\", encoding=\"utf-8\") as all_tokens:\n",
    "amseg = AmharicSegmenter(sent_punct,word_punct)\n",
    "amseg.amharic_tokenizer(text)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read each CSV files and write it to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:05<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# remove output file if it exists, then append each csv to a single file output\n",
    "outfile = \"all_sentences.txt\"\n",
    "os.remove(outfile) if os.path.exists(outfile) else None\n",
    "# all csv files, to controll progress par\n",
    "allnews = []\n",
    "for news in glob.glob(\"data/*.csv\"):\n",
    "    allnews.append(news)\n",
    "\n",
    "# read each files and write to a file system   \n",
    "for news in tqdm.tqdm(allnews,  position=0, leave=True):\n",
    "    data = pd.read_csv(news, names=[\"ID\",\"URL\",\"Date\",\"Media\",\"Content\"] ,encoding=\"utf-8\")\n",
    "    data = data[data.Content.duplicated()==False].reset_index()\n",
    "    text  = data.Content.to_string()\n",
    "    with open (outfile,\"a\", encoding=\"utf-8\") as all_sentences:\n",
    "        for s in amseg.tokenize_sentence(text):\n",
    "            all_sentences.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
