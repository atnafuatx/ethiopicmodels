{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pycld2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sweetviz\n",
    "import random\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import csv\n",
    "import emoji\n",
    "from typing import List\n",
    "from flair.data import Token\n",
    "import sys,os\n",
    "import tqdm\n",
    "import pycld2 as cld2\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./processed/', './data/')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "configParser = configparser.RawConfigParser()   \n",
    "configFilePath = r'config.txt'\n",
    "configParser.read(configFilePath)\n",
    "csvpath = configParser.get('paths', 'csvpath')\n",
    "outpath = configParser.get('paths', 'outpath')\n",
    "outpath,csvpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.amharicSegmenter import AmharicSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url(string): \n",
    "    text = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',string)\n",
    "    return \"\".join(text) # converting return value from list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_eng(string): \n",
    "    text = re.findall('[a-zA-Z0-9]',string)\n",
    "    return \"\".join(text) # converting return value from list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(data):\n",
    "    return data.replace(find_url(data),\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_eng(data):\n",
    "    return ' '.join(re.sub(u\"[^ሀ-፼]\", \" \", data).split()).strip()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_English_letters(content):\n",
    "    for idx,cnt in enumerate(content):\n",
    "        content[idx] = re.sub(r'\\s*[A-Za-z]+\\b','', cnt)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(news):\n",
    "    news = emoji.demojize(news)\n",
    "    re.sub('(:\\S+)(@\\w+)','', news)\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicated_values_data(data):\n",
    "    dup=[]\n",
    "    columns=data.columns\n",
    "    for i in data.columns:\n",
    "        dup.append(sum(data[i].duplicated()))\n",
    "    return pd.concat([pd.Series(columns),pd.Series(dup)],axis=1,keys=['Columns','Duplicate count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_punct = []\n",
    "word_punct = []\n",
    "amseg = AmharicSegmenter(sent_punct,word_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read each CSV files and write it to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ['\\xa087','\\xa0', '|', '/','…','«','\"','“','”','»','-',\n",
    "          '$','%','^','&','*','›','’','‘','‹','<','>','`','´','~','=','+']\n",
    "# remove output file if it exists, then append each csv to a single file output\n",
    "outfile = outpath+\"all_csv_sentences.txt\"\n",
    "os.remove(outfile) if os.path.exists(outfile) else None\n",
    "# all csv files, to controll progress par\n",
    "allnews = []\n",
    "for news in glob.glob(csvpath+\"/*.csv\"):\n",
    "    allnews.append(news)\n",
    "    \n",
    "# read each files and write to a file system \n",
    "\n",
    "with open (outfile,\"a\", encoding=\"utf-8\") as all_sentences:\n",
    "    \n",
    "    lines_seen = set() # holds lines already seen\n",
    "    for news in tqdm.tqdm(allnews,  position=0, leave=True):\n",
    "        data = pd.read_csv(news, names=[\"ID\",\"URL\",\"Date\",\"Media\",\"Content\"] ,encoding=\"utf-8\")\n",
    "        data = data[data.Content.duplicated()==False].reset_index()\n",
    "        #data.Content = data.Content.apply(lambda x: remove_url(x))\n",
    "        #data.Content = data.Content.apply(lambda x: remove_emoji(x))\n",
    "        data.Content = data.Content.apply(lambda x: remove_eng(x))\n",
    "        for token in remove:\n",
    "            data.Content = data.Content.apply(lambda x: x.replace(token,' '))  \n",
    "        for text in data.Content:\n",
    "            for s in amseg.tokenize_sentence(text):\n",
    "                isReliable, textBytesFound, details  = cld2.detect(s)\n",
    "                if details[0][1] =='am' and s not in lines_seen: # check if line is not duplicate:\n",
    "                    all_sentences.write(' '.join(t.text for t in amseg.amharic_tokenizer(s))+'\\n')\n",
    "                    lines_seen.add(s)\n",
    "                           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# downloading online text file to 'data' folder with new name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#download text file from online\n",
    "!wget -O data/amharic1.txt \"https://raw.githubusercontent.com/adtsegaye/Amharic-English-Machine-Translation-Corpus/master/am.txt\"\n",
    "!wget -O data/amharic2.txt  \"https://raw.githubusercontent.com/adtsegaye/Amharic-English-Machine-Translation-Corpus/master/Amharic_English_E-Bible/amharic.txt\"\n",
    "!wget -O data/amharic3.txt \"https://raw.githubusercontent.com/adtsegaye/Amharic-English-Machine-Translation-Corpus/master/Amharic_English_Ethiopic_Bible/amharic.txt\"\n",
    "!wget -O data/amharic4.txt \"https://raw.githubusercontent.com/adtsegaye/Amharic-English-Machine-Translation-Corpus/master/Amharic_English_History/amharic.txt\"\n",
    "!wget -O data/amharic5.txt \"https://raw.githubusercontent.com/adtsegaye/Amharic-English-Machine-Translation-Corpus/master/Amharic_English_JW_Bible/amharic.txt\"\n",
    "!wget -O data/amharic6.txt \"https://raw.githubusercontent.com/adtsegaye/Amharic-English-Machine-Translation-Corpus/master/Amharic_English_JW_Daily_Quote/amharic.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading zipped files to data filder with renamed file name and in gz format\n",
    "!wget -O data/am1.txt.zip \"https://object.pouta.csc.fi/OPUS-Tanzil/v1/mono/am.txt.gz\" \n",
    "!wget -O data/am2.txt.zip \"https://object.pouta.csc.fi/OPUS-JW300/v1b/mono/am.txt.gz\"\n",
    "!wget -O data/am3.txt.zip \"https://object.pouta.csc.fi/OPUS-GNOME/v1/mono/am.txt.gz\"\n",
    "!wget -O data/am4.txt.zip \"https://object.pouta.csc.fi/OPUS-bible-uedin/v1/mono/am.tok.gz\"\n",
    "!wget -O data/am5.txt.zip \"https://object.pouta.csc.fi/OPUS-Tanzil/v1/mono/am.txt.gz\"\n",
    "!wget -O data/am6.txt.zip \"https://object.pouta.csc.fi/OPUS-Mozilla-I10n/v1/mono/am.tok.gz\"\n",
    "!wget -O data/am7.txt.zip \"https://object.pouta.csc.fi/OPUS-TED2020/v1/mono/am.txt.gz\"\n",
    "!wget -O data/am8.txt.zip \"https://object.pouta.csc.fi/OPUS-GlobalVoices/v2018q4/mono/am.txt.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from zipfile import ZipFile\n",
    "#with ZipFile('data/am8.txt.zip', 'r') as my_zip:\n",
    "#    my_zip.extractall('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unzip all zipped files on data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing working directory to 'data' folder that contain all zipped files\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "#search_path = os.getcwd()\n",
    "search_path = os.chdir('data')#change work directory\n",
    "\n",
    "file_type = \".gz\"\n",
    "for fname in os.listdir(path =search_path):\n",
    "    if fname.endswith(file_type):\n",
    "        with gzip.open(fname,'rb') as f_in:\n",
    "            with open(fname+'.txt','wb') as f_out:\n",
    "                shutil.copyfileobj(f_in,f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return to back=>normalization directory\n",
    "os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merging online text files as 'all_online_sentences' & save to outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outfile = csvpath+\"all_online_sentences.txt\"\n",
    "os.remove(outfile) if os.path.exists(outfile) else None\n",
    "with open (outfile,\"a\", encoding=\"utf-8\") as result:\n",
    "    \n",
    "    for file in glob.glob(outpath+\"/*.txt\"):\n",
    "        with open(file,encoding=\"utf-8\") as infile:\n",
    "            for line in infile:\n",
    "                result.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess merged online text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outfile = outpath+\"all_online_sentences.txt\"\n",
    "os.remove(outfile) if os.path.exists(outfile) else None\n",
    "\n",
    "with open (outfile,\"a\", encoding=\"utf-8\") as all_online_sentences:\n",
    "    lines_seen = set() # holds lines already seen\n",
    "    data = pd.read_csv(csvpath+\"all_online_sentences.txt\",names=[\"Content\"] ,encoding=\"utf-8\")#file that we get from colab\n",
    "    data = data[data.Content.duplicated()==False].reset_index()\n",
    "    data.Content = data.Content.apply(lambda x: remove_eng(x))\n",
    "    for token in remove:\n",
    "        data.Content = data.Content.apply(lambda x: x.replace(token,' '))  \n",
    "    for text in data.Content:\n",
    "        for s in amseg.tokenize_sentence(text):\n",
    "            isReliable, textBytesFound, details  = cld2.detect(s)\n",
    "            if details[0][1] =='am' and s not in lines_seen: # check if line is not duplicate:\n",
    "                all_online_sentences.write(' '.join(t.text for t in amseg.amharic_tokenizer(s))+'\\n')\n",
    "                lines_seen.add(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merging preprocessed csv text file and online text file to 'all_in_one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = outpath+\"all_in_one.txt\"\n",
    "os.remove(outfile) if os.path.exists(outfile) else None\n",
    "with open (outfile,\"a\", encoding=\"utf-8\") as result:\n",
    "    \n",
    "    for file in glob.glob(outpath+\"/*.txt\"):\n",
    "        with open(file,encoding=\"utf-8\") as infile:\n",
    "            for line in infile:\n",
    "                result.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# devide text file to train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outpath+\"all_in_one.txt\",'r',encoding=\"utf-8\")as line:\n",
    "    no_of_line = line.read().splitlines()\n",
    "len(no_of_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "with open(outpath+\"all_in_one.txt\", \"r\",encoding=\"utf-8\") as f:    \n",
    "    data = f.read().split('\\n')\n",
    "    random.shuffle(data)\n",
    "#Assume we have 117552 line of text and 90:10 ratio for  train and test respectivily\n",
    "    test_data = data[:11755]\n",
    "    train_data = data[11755:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = [\"snet1\", \"sent2\", \"sent3\",\"sent4\",\"sent5\",\"sent6\",\"sent7\",\"sent8\",\"sent9\",\"sent10\"]\n",
    "#random.shuffle(data)\n",
    "#print(data[:len(data)*0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
