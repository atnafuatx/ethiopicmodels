{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import sweetviz\n",
    "import random\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import unidecode\n",
    "import numpy as np\n",
    "import csv\n",
    "import emoji\n",
    "from typing import List\n",
    "from flair.data import Token\n",
    "import sys,os\n",
    "import tqdm\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.amharicSegmenter import AmharicSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url(string): \n",
    "    text = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',string)\n",
    "    return \"\".join(text) # converting return value from list to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(data):\n",
    "    return data.replace(find_url(data),\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(news):\n",
    "    for idx,cnt in enumerate(news):\n",
    "        news[idx] = re.sub(r'(:\\S+)(@\\w+)[A-Za-z]','', cnt)\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicated_values_data(data):\n",
    "    dup=[]\n",
    "    columns=data.columns\n",
    "    for i in data.columns:\n",
    "        dup.append(sum(data[i].duplicated()))\n",
    "    return pd.concat([pd.Series(columns),pd.Series(dup)],axis=1,keys=['Columns','Duplicate count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Removing Duplicated Sentences\n",
      "   Columns  Duplicate count\n",
      "0  Content              280\n",
      "======================================== \n",
      "After Removing Duplicated Sentences\n",
      "   Columns  Duplicate count\n",
      "0  Content                0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_punct = []\n",
    "word_punct = []\n",
    "amseg = AmharicSegmenter(sent_punct,word_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read each CSV files and write it to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'amseg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-15a5a2f6cd65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtext\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mContent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mall_sentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mamseg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mall_sentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'amseg' is not defined"
     ]
    }
   ],
   "source": [
    "remove = ['\\xa087','\\xa0', '|', '/','…','«','\"','“','”','»','-',\n",
    "          '$','%','^','&','*','›','’','‘','‹','<','>','`','´','~','=','+']\n",
    "# remove output file if it exists, then append each csv to a single file output\n",
    "outfile = \"all_sentences.txt\"\n",
    "os.remove(outfile) if os.path.exists(outfile) else None\n",
    "# all csv files, to controll progress par\n",
    "allnews = []\n",
    "for news in glob.glob(\"data/*.csv\"):\n",
    "    allnews.append(news)\n",
    "\n",
    "# read each files and write to a file system   \n",
    "for news in tqdm.tqdm(allnews,  position=0, leave=True):\n",
    "    data = pd.read_csv(news, names=[\"ID\",\"URL\",\"Date\",\"Media\",\"Content\"] ,encoding=\"utf-8\")\n",
    "    data = data[data.Content.duplicated()==False].reset_index()\n",
    "    data.Content = data.Content.apply(lambda x: remove_url(x))\n",
    "    for token in remove:\n",
    "        data.Content = data.Content.apply(lambda x: x.replace(token,' '))  \n",
    "    text  = data.Content.to_string()\n",
    "    with open (outfile,\"a\", encoding=\"utf-8\") as all_sentences:\n",
    "        for s in amseg.tokenize_sentence(text):\n",
    "            all_sentences.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
