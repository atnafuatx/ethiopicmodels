{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from flair.data import Token\n",
    "class AmharicSegmenter:\n",
    "    SENT_PUNC =  []\n",
    "    WORD_PUNC =  []\n",
    "    def __init__(self, sent_punct, word_punct):\n",
    "        if sent_punct:\n",
    "            self.SENT_PUNC = sent_punct\n",
    "        else:\n",
    "            self.SENT_PUNC = [\"።\",\"፥\",\"፨\",\"::\",\"፡፡\",\"?\",\"!\"]\n",
    "        if word_punct:\n",
    "            self.WORD_PUNC = word_punct\n",
    "        else:\n",
    "            self.WORD_PUNC =  [\"።\",\"፥\",\"፤\",\"፨\",\"?\",\"!\",\":\",\"፡\",\"፦\",\"፣\"]\n",
    "            \n",
    "    def amharic_tokenizer(self, text: str) -> List[Token]:\n",
    "        \"\"\"\n",
    "        Tokenizer based on space character and different Amharic punctuation marksonly.\n",
    "        \"\"\"\n",
    "        tokens: List[Token] = []\n",
    "        word = \"\"\n",
    "        index = -1\n",
    "        previchar = ''\n",
    "        for index, char in enumerate(text):\n",
    "            if char == \" \":\n",
    "                if len(word) > 0:\n",
    "                    start_position = index - len(word)\n",
    "                    tokens.append(\n",
    "                        Token(\n",
    "                            text=word, start_position=start_position, whitespace_after=True\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                word = \"\"\n",
    "            elif char in self.WORD_PUNC:\n",
    "                if len(word) > 0 and previchar != char:\n",
    "                    start_position = index - len(word)\n",
    "                    tokens.append(\n",
    "                        Token(\n",
    "                            text=word, start_position=start_position, whitespace_after=True\n",
    "                        )\n",
    "                    )\n",
    "                    word = \"\"\n",
    "                previchar = char\n",
    "                word += char\n",
    "\n",
    "            else:\n",
    "                word += char\n",
    "        # increment for last token in sentence if not followed by whitespace\n",
    "        index += 1\n",
    "        if len(word) > 0:\n",
    "            start_position = index - len(word)\n",
    "            tokens.append(\n",
    "                Token(text=word, start_position=start_position, whitespace_after=False)\n",
    "            )\n",
    "        return tokens\n",
    "    \n",
    "    def find_all(self, punct, text):\n",
    "        return [i + len(punct)-1 for i in range(len(text)) if text.startswith(punct, i)]\n",
    "        \n",
    "\n",
    "    def tokenize_sentence(self, text: str):\n",
    "        text = re.sub(\"\\s+\", \" \",text)\n",
    "        text = re.sub(\"\\n\", \"።\",text)\n",
    "        tokenized_text = []\n",
    "        idxs = [-1] # see below, used to start the next sentence after the index of the sentence segmenter\n",
    "        for sep in self.SENT_PUNC:\n",
    "            idx = text.find(sep)\n",
    "            if idx > 0:\n",
    "                allidx = self.find_all(sep, text)\n",
    "                for idx in allidx:\n",
    "                    idxs.append(idx)\n",
    "        idxs = sorted(idxs)\n",
    "        if len(idxs) ==1:\n",
    "            tokenized_text.append(text)# just one sentence without the punctuation marks\n",
    "        for i in range(len(idxs)-1): # \n",
    "            tokenized_text.append(text[idxs[i]+1:idxs[i+1]+1].strip())\n",
    "        return tokenized_text\n",
    "\n",
    "    # apply sentence tokenization\n",
    "    def window_lines(self, line, window):\n",
    "        '''\n",
    "        Some models require the length of the sentence to be moderate, so to avoid shorter sentences, append some using some windowing technique\n",
    "        '''\n",
    "        try:\n",
    "            text = re.sub(\"\\s+\", \" \",line)\n",
    "            sentences = [s for s in self.tokenize_sentence(text) if len(s) >= 6]\n",
    "            windowed_sentences = []\n",
    "            for snt in range(len(sentences)):\n",
    "                windowed_sentences.append(\" \".join(sentences[snt: snt + window]))\n",
    "            return windowed_sentences\n",
    "        except:\n",
    "            # print(f\"Could not parse line \\n{line}\\n\")\n",
    "            return []\n",
    "        \n",
    "    # windwoing with segmented sentence\n",
    "    def window_sents(self, sentences, window):\n",
    "        '''\n",
    "        Some models require the length of the sentence to be moderate, so to avoid shorter sentences, append some using some windowing technique\n",
    "        '''\n",
    "        try:\n",
    "            windowed_sentences = []\n",
    "            for snum in range(len(sentences)):\n",
    "                windowed_sentences.append(\" \".join(sentences[snum: snum + window]))\n",
    "            return windowed_sentences\n",
    "        except:\n",
    "            # print(f\"Could not parse line \\n{line}\\n\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
